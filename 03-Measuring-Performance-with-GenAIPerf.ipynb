{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb276c1",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59831a4d",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**Notebook 3:** Measuring NIM Performance with GenAI-Perf</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e94b2",
   "metadata": {},
   "source": [
    "In this notebook, you will measure the performance of our NIM instance using the [**NVIDIA GenAI-Perf**](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/client/src/c%2B%2B/perf_analyzer/genai-perf/README.html) tool. It is a client-side LLM-focused benchmarking tool which can test for key metrics such as `TTFT`, `ITL`, `E2E Latency`, and requests per second. GenAI-Perf can be used to measure and compare the performance of any inference endpoint that provides an OpenAI-compatible or Triton Inference Server API. Read [**our documentation**](https://docs.nvidia.com/nim/benchmarking/llm/latest/step-by-step.html) to gain more insights about NVIDIA GenAI-Perf.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Use NVIDIA GenAI-Perf to measure key performance metrics of an NIM LLM deployment.\n",
    "- Understand the impact of different input and output lengths on performance.\n",
    "- Analyze the effect of concurrency on latency and throughput.\n",
    "- Estimate the required number of GPUs for a given workload.\n",
    "\n",
    "**Before starting this notebook, please make sure to watch its corresponding video.**\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [**First Performance Measurement with NVIDIA GenAI-Perf**](#First-Performance-Measurement-with-NVIDIA-GenAI-Perf)\n",
    "- [**Loop Over Concurrencies with NVIDIA GenAI-Perf**](#Loop-Over-Concurrencies-with-NVIDIA-GenAI-Perf)\n",
    "- [**Plot Latency-Throughput Curves**](#Plot-Latency-Throughput-Curves)\n",
    "- [**[EXERCISE] Calculate the Necessary Number of GPUs**](#[EXERCISE]-Calculate-the-Necessary-Number-of-GPUs)\n",
    "\n",
    "## Notebook Source\n",
    "This notebook is part of the **NVIDIA Deep Learning Institute (DLI)** curriculum. You can find more information and additional resources at the [**NVIDIA DLI website**](https://www.nvidia.com/en-us/training/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2cf4f",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **First Performance Measurement with NVIDIA GenAI-Perf**\n",
    "\n",
    "Start by submitting a simple call to [**NVIDIA GenAI-Perf**](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/client/src/c%2B%2B/perf_analyzer/genai-perf/README.html). Though it is a CLI tool aimed to be run in the terminal, the commands can also be invoked from your notebook. Start by passing in a few variables to our experimental setup:\n",
    "\n",
    "- To create a variety of inputs with some realistic sizes, you can sample synthetic inputs with defined input statistics (mean and standard deviation of length). \n",
    "- To keep the output length consistent for your experiments, you can set `min_tokens = max_tokens = output-tokens-mean` and specify `ignore_eos: true` to tell the backend to ignore the special `EOS` tokens (typical stopping criteria for the LLM). \n",
    "- To test out how your system will operate with multiple concurrent users/asynchronous calls, you can specify the `concurrency` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb74e123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-31 17:53 [INFO] genai_perf.parser:711 - Detected passthrough args: ['--max-threads=256']\n",
      "2024-10-31 17:53 [INFO] genai_perf.parser:226 - Model name 'meta/llama3-8b-instruct' cannot be used to create artifact directory. Instead, 'meta_llama3-8b-instruct' will be used.\n",
      "2024-10-31 17:53 [INFO] genai_perf.wrapper:138 - Running Perf Analyzer : 'perf_analyzer -m meta/llama3-8b-instruct --async --input-data artifacts/meta_llama3-8b-instruct-openai-chat-concurrency10/llm_inputs.json --endpoint v1/chat/completions --service-kind openai -u nim:8000 --measurement-interval 10000 --stability-percentage 999 --profile-export-file artifacts/meta_llama3-8b-instruct-openai-chat-concurrency10/profile_export.json -i http --concurrency-range 10 --max-threads=256'\n",
      "Error while terminating subprocess (pid=486): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/genai-perf\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/genai_perf/main.py\", line 154, in main\n",
      "    run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/genai_perf/main.py\", line 143, in run\n",
      "    args.func(args, extra_args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/genai_perf/parser.py\", line 665, in profile_handler\n",
      "    Profiler.run(args=args, extra_args=extra_args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/genai_perf/wrapper.py\", line 142, in run\n",
      "    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 505, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1146, in communicate\n",
      "    self.wait()\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export INPUT_SEQUENCE_LENGTH=200\n",
    "export INPUT_SEQUENCE_STD=10\n",
    "export OUTPUT_SEQUENCE_LENGTH=50\n",
    "export CONCURRENCY=10\n",
    "\n",
    "genai-perf \\\n",
    "    -m meta/llama3-8b-instruct \\\n",
    "    --endpoint-type chat \\\n",
    "    --service-kind openai \\\n",
    "    --streaming \\\n",
    "    -u nim:8000 \\\n",
    "    --synthetic-input-tokens-mean $INPUT_SEQUENCE_LENGTH \\\n",
    "    --synthetic-input-tokens-stddev $INPUT_SEQUENCE_STD \\\n",
    "    --concurrency $CONCURRENCY \\\n",
    "    --output-tokens-mean $OUTPUT_SEQUENCE_LENGTH \\\n",
    "    --extra-inputs max_tokens:$OUTPUT_SEQUENCE_LENGTH \\\n",
    "    --extra-inputs min_tokens:$OUTPUT_SEQUENCE_LENGTH \\\n",
    "    --extra-inputs ignore_eos:true \\\n",
    "    -- \\\n",
    "    --max-threads=256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59415b76",
   "metadata": {},
   "source": [
    "Upon running successfully, you should see a table of results with statistical measurements of the latencies and throughputs. The inflight batching mechanism of NIM, together with the standard deviation in the input length, leads to batches with both prefill and decoding stages. This means that each sequence has a different measurement of latency and throughput.\n",
    "\n",
    "We encourage you to play with the input and output lengths, as well as with the concurrency. Do you see similar trends as the ones explored in the previous notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161a17d",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Loop Over Concurrencies with NVIDIA GenAI-Perf**\n",
    "\n",
    "In the previous notebook, you analyzed the impact of the concurrency level by displaying plots of the latency and throughput. With NVIDIA GenAI-Perf, you can recreate those plots by measuring the performance of NIM with different concurrencies. Let's start by defining a list of concurrencies to iterate over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e24ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a list with the concurrencies\n",
    "concurrencies = [1, 2, 5, 10, 50, 100, 250]\n",
    "\n",
    "# Set the environment variable in Python\n",
    "os.environ['CONCURRENCIES'] = ','.join(map(str, concurrencies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967dda1e",
   "metadata": {},
   "source": [
    "The next step is to iterate over the list of concurrencies. The following bash command loops over the concurrencies and saves the results under the `artifacts` directory. It takes around 1m20s in our setup, so you can start running it while inspecting the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8dcd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrency: 1\n",
      "2024-10-31 17:54 [INFO] genai_perf.parser:711 - Detected passthrough args: ['--max-threads=256']\n",
      "2024-10-31 17:54 [INFO] genai_perf.parser:226 - Model name 'meta/llama3-8b-instruct' cannot be used to create artifact directory. Instead, 'meta_llama3-8b-instruct' will be used.\n",
      "2024-10-31 17:54 [INFO] genai_perf.wrapper:138 - Running Perf Analyzer : 'perf_analyzer -m meta/llama3-8b-instruct --async --input-data artifacts/meta_llama3-8b-instruct-openai-chat-concurrency1/llm_inputs.json --endpoint v1/chat/completions --service-kind openai -u nim:8000 --measurement-interval 10000 --stability-percentage 999 --profile-export-file artifacts/meta_llama3-8b-instruct-openai-chat-concurrency1/200_50.json -i http --concurrency-range 1 --max-threads=256'\n",
      "\u001b[3m                                  LLM Metrics                                   \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m               Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   avg\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   min\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   max\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p99\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   p90\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p75\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36mTime to first token (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 24.62\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 23.59\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 26.31\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m26.27\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 25.36\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m24.96\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mInter token latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 11.38\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m  9.88\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 12.86\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m12.78\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 12.43\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m11.86\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m    Request latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m669.58\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m664.09\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m675.84\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m675.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m674.30\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m673.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m  Output sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 57.89\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 51.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 66.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m64.96\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 62.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m61.00\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m   Input sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.02\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m201.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "└──────────────────────────┴────────┴────────┴────────┴───────┴────────┴───────┘\n",
      "Output token throughput (per sec): 86.44\n",
      "Request throughput (per sec): 1.49\n",
      "2024-10-31 17:54 [INFO] genai_perf.export_data.json_exporter:56 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency1/profile_export_genai_perf.json\n",
      "2024-10-31 17:54 [INFO] genai_perf.export_data.csv_exporter:69 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency1/profile_export_genai_perf.csv\n",
      "Concurrency: 2\n",
      "2024-10-31 17:54 [INFO] genai_perf.parser:711 - Detected passthrough args: ['--max-threads=256']\n",
      "2024-10-31 17:54 [INFO] genai_perf.parser:226 - Model name 'meta/llama3-8b-instruct' cannot be used to create artifact directory. Instead, 'meta_llama3-8b-instruct' will be used.\n",
      "2024-10-31 17:54 [INFO] genai_perf.wrapper:138 - Running Perf Analyzer : 'perf_analyzer -m meta/llama3-8b-instruct --async --input-data artifacts/meta_llama3-8b-instruct-openai-chat-concurrency2/llm_inputs.json --endpoint v1/chat/completions --service-kind openai -u nim:8000 --measurement-interval 10000 --stability-percentage 999 --profile-export-file artifacts/meta_llama3-8b-instruct-openai-chat-concurrency2/200_50.json -i http --concurrency-range 2 --max-threads=256'\n",
      "\u001b[3m                                  LLM Metrics                                   \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m               Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   avg\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   min\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   max\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p99\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   p90\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p75\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36mTime to first token (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 35.04\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 33.12\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 41.62\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m41.29\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 40.20\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m34.56\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mInter token latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 11.46\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 10.09\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 12.72\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m12.71\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 12.19\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m11.80\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m    Request latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m687.80\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m679.62\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m698.12\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m698.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m692.93\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m691.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m  Output sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 58.12\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 52.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 66.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m64.97\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 63.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m61.00\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m   Input sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.03\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m202.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "└──────────────────────────┴────────┴────────┴────────┴───────┴────────┴───────┘\n",
      "Output token throughput (per sec): 168.99\n",
      "Request throughput (per sec): 2.91\n",
      "2024-10-31 17:55 [INFO] genai_perf.export_data.json_exporter:56 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency2/profile_export_genai_perf.json\n",
      "2024-10-31 17:55 [INFO] genai_perf.export_data.csv_exporter:69 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency2/profile_export_genai_perf.csv\n",
      "Concurrency: 5\n",
      "2024-10-31 17:55 [INFO] genai_perf.parser:711 - Detected passthrough args: ['--max-threads=256']\n",
      "2024-10-31 17:55 [INFO] genai_perf.parser:226 - Model name 'meta/llama3-8b-instruct' cannot be used to create artifact directory. Instead, 'meta_llama3-8b-instruct' will be used.\n",
      "2024-10-31 17:55 [INFO] genai_perf.wrapper:138 - Running Perf Analyzer : 'perf_analyzer -m meta/llama3-8b-instruct --async --input-data artifacts/meta_llama3-8b-instruct-openai-chat-concurrency5/llm_inputs.json --endpoint v1/chat/completions --service-kind openai -u nim:8000 --measurement-interval 10000 --stability-percentage 999 --profile-export-file artifacts/meta_llama3-8b-instruct-openai-chat-concurrency5/200_50.json -i http --concurrency-range 5 --max-threads=256'\n",
      "\u001b[3m                                  LLM Metrics                                   \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m               Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   avg\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   min\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   max\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p99\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   p90\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p75\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36mTime to first token (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 79.30\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 25.92\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 97.79\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m93.21\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 80.24\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m79.75\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mInter token latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 11.88\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 10.28\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 13.33\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m13.31\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 12.78\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m12.34\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m    Request latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m757.84\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m754.86\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m778.55\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m777.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m760.42\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m758.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m  Output sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 58.29\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 52.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 67.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m65.66\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 63.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m61.00\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m   Input sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.03\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m202.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m201.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "└──────────────────────────┴────────┴────────┴────────┴───────┴────────┴───────┘\n",
      "Output token throughput (per sec): 384.44\n",
      "Request throughput (per sec): 6.60\n",
      "2024-10-31 17:56 [INFO] genai_perf.export_data.json_exporter:56 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency5/profile_export_genai_perf.json\n",
      "2024-10-31 17:56 [INFO] genai_perf.export_data.csv_exporter:69 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency5/profile_export_genai_perf.csv\n",
      "Concurrency: 10\n",
      "2024-10-31 17:56 [INFO] genai_perf.parser:711 - Detected passthrough args: ['--max-threads=256']\n",
      "2024-10-31 17:56 [INFO] genai_perf.parser:226 - Model name 'meta/llama3-8b-instruct' cannot be used to create artifact directory. Instead, 'meta_llama3-8b-instruct' will be used.\n",
      "2024-10-31 17:56 [INFO] genai_perf.wrapper:138 - Running Perf Analyzer : 'perf_analyzer -m meta/llama3-8b-instruct --async --input-data artifacts/meta_llama3-8b-instruct-openai-chat-concurrency10/llm_inputs.json --endpoint v1/chat/completions --service-kind openai -u nim:8000 --measurement-interval 10000 --stability-percentage 999 --profile-export-file artifacts/meta_llama3-8b-instruct-openai-chat-concurrency10/200_50.json -i http --concurrency-range 10 --max-threads=256'\n",
      "\u001b[3m                                  LLM Metrics                                   \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m               Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   avg\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   min\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   max\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p99\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   p90\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p75\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36mTime to first token (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m145.53\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 26.05\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m164.83\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m162.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m155.16\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m150.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mInter token latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 12.75\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 10.84\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 15.99\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m15.32\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 13.74\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m13.22\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m    Request latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m878.52\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m871.03\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m892.71\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m892.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m887.82\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m879.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m  Output sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 58.67\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 51.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 70.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m67.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 63.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m61.00\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m   Input sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.03\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m202.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m201.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "└──────────────────────────┴────────┴────────┴────────┴───────┴────────┴───────┘\n",
      "Output token throughput (per sec): 667.56\n",
      "Request throughput (per sec): 11.38\n",
      "2024-10-31 17:56 [INFO] genai_perf.export_data.json_exporter:56 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency10/profile_export_genai_perf.json\n",
      "2024-10-31 17:56 [INFO] genai_perf.export_data.csv_exporter:69 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency10/profile_export_genai_perf.csv\n",
      "Concurrency: 50\n",
      "2024-10-31 17:56 [INFO] genai_perf.parser:711 - Detected passthrough args: ['--max-threads=256']\n",
      "2024-10-31 17:56 [INFO] genai_perf.parser:226 - Model name 'meta/llama3-8b-instruct' cannot be used to create artifact directory. Instead, 'meta_llama3-8b-instruct' will be used.\n",
      "2024-10-31 17:56 [INFO] genai_perf.wrapper:138 - Running Perf Analyzer : 'perf_analyzer -m meta/llama3-8b-instruct --async --input-data artifacts/meta_llama3-8b-instruct-openai-chat-concurrency50/llm_inputs.json --endpoint v1/chat/completions --service-kind openai -u nim:8000 --measurement-interval 10000 --stability-percentage 999 --profile-export-file artifacts/meta_llama3-8b-instruct-openai-chat-concurrency50/200_50.json -i http --concurrency-range 50 --max-threads=256'\n",
      "\u001b[3m                                  LLM Metrics                                   \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m               Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   avg\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   min\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   max\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p99\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   p90\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p75\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36mTime to first token (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m627.34\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m100.81\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m837.75\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m837.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m712.98\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m612.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mInter token latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 22.48\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 17.99\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 34.51\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m26.96\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 24.68\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m23.54\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m    Request latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1,914…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1,868…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m2,019…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m2,01…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m2,013…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1,94…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m  Output sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 58.44\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 51.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 70.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m66.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 63.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m61.00\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m   Input sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.03\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m202.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m201.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "└──────────────────────────┴────────┴────────┴────────┴───────┴────────┴───────┘\n",
      "Output token throughput (per sec): 1525.06\n",
      "Request throughput (per sec): 26.10\n",
      "2024-10-31 17:57 [INFO] genai_perf.export_data.json_exporter:56 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency50/profile_export_genai_perf.json\n",
      "2024-10-31 17:57 [INFO] genai_perf.export_data.csv_exporter:69 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency50/profile_export_genai_perf.csv\n",
      "Concurrency: 100\n",
      "2024-10-31 17:57 [INFO] genai_perf.parser:711 - Detected passthrough args: ['--max-threads=256']\n",
      "2024-10-31 17:57 [INFO] genai_perf.parser:226 - Model name 'meta/llama3-8b-instruct' cannot be used to create artifact directory. Instead, 'meta_llama3-8b-instruct' will be used.\n",
      "2024-10-31 17:57 [INFO] genai_perf.wrapper:138 - Running Perf Analyzer : 'perf_analyzer -m meta/llama3-8b-instruct --async --input-data artifacts/meta_llama3-8b-instruct-openai-chat-concurrency100/llm_inputs.json --endpoint v1/chat/completions --service-kind openai -u nim:8000 --measurement-interval 10000 --stability-percentage 999 --profile-export-file artifacts/meta_llama3-8b-instruct-openai-chat-concurrency100/200_50.json -i http --concurrency-range 100 --max-threads=256'\n",
      "\u001b[3m                                  LLM Metrics                                   \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m               Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   avg\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   min\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   max\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p99\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   p90\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p75\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36mTime to first token (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1,019…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m118.95\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1,564…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1,56…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1,419…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1,30…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mInter token latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 38.61\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 26.09\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 59.67\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m50.89\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 47.21\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m44.38\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m    Request latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3,228…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3,092…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3,305…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3,30…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3,296…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3,29…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m  Output sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 58.38\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 51.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 70.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m66.01\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 63.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m60.00\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m   Input sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.03\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m202.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m201.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "└──────────────────────────┴────────┴────────┴────────┴───────┴────────┴───────┘\n",
      "Output token throughput (per sec): 1806.07\n",
      "Request throughput (per sec): 30.94\n",
      "2024-10-31 17:58 [INFO] genai_perf.export_data.json_exporter:56 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency100/profile_export_genai_perf.json\n",
      "2024-10-31 17:58 [INFO] genai_perf.export_data.csv_exporter:69 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency100/profile_export_genai_perf.csv\n",
      "Concurrency: 250\n",
      "2024-10-31 17:58 [INFO] genai_perf.parser:711 - Detected passthrough args: ['--max-threads=256']\n",
      "2024-10-31 17:58 [INFO] genai_perf.parser:226 - Model name 'meta/llama3-8b-instruct' cannot be used to create artifact directory. Instead, 'meta_llama3-8b-instruct' will be used.\n",
      "2024-10-31 17:58 [INFO] genai_perf.wrapper:138 - Running Perf Analyzer : 'perf_analyzer -m meta/llama3-8b-instruct --async --input-data artifacts/meta_llama3-8b-instruct-openai-chat-concurrency250/llm_inputs.json --endpoint v1/chat/completions --service-kind openai -u nim:8000 --measurement-interval 10000 --stability-percentage 999 --profile-export-file artifacts/meta_llama3-8b-instruct-openai-chat-concurrency250/200_50.json -i http --concurrency-range 250 --max-threads=256'\n",
      "\u001b[3m                                  LLM Metrics                                   \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m               Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   avg\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   min\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   max\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p99\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   p90\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  p75\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36mTime to first token (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m2,156…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 97.10\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3,691…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3,68…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3,345…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3,13…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mInter token latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 88.34\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 53.95\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m130.47\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m123.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m112.46\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m102.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m    Request latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m7,223…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m6,953…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m7,431…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m7,42…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m7,423…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m7,34…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m  Output sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 58.56\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 51.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 69.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m67.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 63.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m61.00\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m   Input sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.03\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m202.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m201.…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m200.…\u001b[0m\u001b[32m \u001b[0m│\n",
      "└──────────────────────────┴────────┴────────┴────────┴───────┴────────┴───────┘\n",
      "Output token throughput (per sec): 2019.22\n",
      "Request throughput (per sec): 34.48\n",
      "2024-10-31 17:58 [INFO] genai_perf.export_data.json_exporter:56 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency250/profile_export_genai_perf.json\n",
      "2024-10-31 17:58 [INFO] genai_perf.export_data.csv_exporter:69 - Generating artifacts/meta_llama3-8b-instruct-openai-chat-concurrency250/profile_export_genai_perf.csv\n",
      "CPU times: user 68.5 ms, sys: 15.4 ms, total: 83.9 ms\n",
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "export INPUT_SEQUENCE_LENGTH=200\n",
    "export INPUT_SEQUENCE_STD=0\n",
    "export OUTPUT_SEQUENCE_LENGTH=50\n",
    "\n",
    "IFS=',' read -r -a array <<< \"$CONCURRENCIES\"\n",
    "for concurrency in \"${array[@]}\"; do\n",
    "\n",
    "    echo \"Concurrency: $concurrency\"\n",
    "    ## TODO: Invoke genai-perf for each concurrency level.\n",
    "    ## TODO: Additionally, set measurement interval to 10000 to avoid timeout\n",
    "    ## TODO: To differentiate the logfile, modify the --profile-export-file to something else. \n",
    "    genai-perf \\\n",
    "        -m meta/llama3-8b-instruct \\\n",
    "        --endpoint-type chat \\\n",
    "        --service-kind openai \\\n",
    "        --streaming \\\n",
    "        -u nim:8000 \\\n",
    "        --synthetic-input-tokens-mean $INPUT_SEQUENCE_LENGTH \\\n",
    "        --synthetic-input-tokens-stddev $INPUT_SEQUENCE_STD \\\n",
    "        --concurrency $concurrency \\\n",
    "        --output-tokens-mean $OUTPUT_SEQUENCE_LENGTH \\\n",
    "        --extra-inputs max_tokens:$OUTPUT_SEQUENCE_LENGTH \\\n",
    "        --extra-inputs min_tokens:$OUTPUT_SEQUENCE_LENGTH \\\n",
    "        --extra-inputs ignore_eos:true \\\n",
    "        --measurement-interval 10000 \\\n",
    "        --profile-export-file ${INPUT_SEQUENCE_LENGTH}_${OUTPUT_SEQUENCE_LENGTH}.json \\\n",
    "        -- \\\n",
    "        --max-threads=256 \\\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58944f9c",
   "metadata": {},
   "source": [
    "Take a look at the `artifact` directory to understand how NVIDIA GenAI-Perf saves the results. Each concurrency has a subdirectory, and inside you can find the file `profile_export_genai_perf.json` with a summary of the performance metrics.\n",
    "\n",
    "<details>\n",
    "<summary><b>Reveal Solution</b></summary>\n",
    "\n",
    "```sh\n",
    "%%time\n",
    "%%bash\n",
    "export INPUT_SEQUENCE_LENGTH=200\n",
    "export INPUT_SEQUENCE_STD=0\n",
    "export OUTPUT_SEQUENCE_LENGTH=50\n",
    "\n",
    "IFS=',' read -r -a array <<< \"$CONCURRENCIES\"\n",
    "for concurrency in \"${array[@]}\"; do\n",
    "\n",
    "    echo \"Concurrency: $concurrency\"\n",
    "    ## TODO: Invoke genai-perf for each concurrency level.\n",
    "    ## TODO: Additionally, set measurement interval to 10000 to avoid timeout\n",
    "    ## TODO: To differentiate the logfile, modify the --profile-export-file to something else. \n",
    "    genai-perf \\\n",
    "        -m meta/llama3-8b-instruct \\\n",
    "        --endpoint-type chat \\\n",
    "        --service-kind openai \\\n",
    "        --streaming \\\n",
    "        -u nim:8000 \\\n",
    "        --synthetic-input-tokens-mean $INPUT_SEQUENCE_LENGTH \\\n",
    "        --synthetic-input-tokens-stddev $INPUT_SEQUENCE_STD \\\n",
    "        --concurrency $concurrency \\\n",
    "        --output-tokens-mean $OUTPUT_SEQUENCE_LENGTH \\\n",
    "        --extra-inputs max_tokens:$OUTPUT_SEQUENCE_LENGTH \\\n",
    "        --extra-inputs min_tokens:$OUTPUT_SEQUENCE_LENGTH \\\n",
    "        --extra-inputs ignore_eos:true \\\n",
    "        --measurement-interval 10000 \\\n",
    "        --profile-export-file ${INPUT_SEQUENCE_LENGTH}_${OUTPUT_SEQUENCE_LENGTH}.json \\\n",
    "        -- \\\n",
    "        --max-threads=256 \\\n",
    "\n",
    "done\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d575c1",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Plot Latency-Throughput Curves**\n",
    "\n",
    "Now that the artifacts have been generated, you can read the values for `TTFT` and output tokens per second from the various concurrencies. Run the following cell to read the `profile_export_genai_perf.json` file for each concurrency and accumulate the output-tokens-per-second and time-to-first-token measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fbdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_data(file_path, metric):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        return data[metric]['avg']\n",
    "\n",
    "root_dir = \"./artifacts\"\n",
    "directory_prefix = \"meta_llama3-8b-instruct-openai-chat-concurrency\"\n",
    "TPS = []\n",
    "TTFT = []\n",
    "for con in concurrencies:\n",
    "    file = os.path.join(root_dir, directory_prefix+str(con), f\"profile_export_genai_perf.json\")\n",
    "    TPS.append(parse_data(file, \"output_token_throughput\"))\n",
    "    TTFT.append(parse_data(file, \"time_to_first_token\")/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2496ca",
   "metadata": {},
   "source": [
    "Let's use plotly to depict their relationship with respect to concurrency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc9e01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\"\n",
    "\n",
    "fig = px.line(x=TTFT, y=TPS, text=concurrencies)\n",
    "fig.update_traces(textposition=\"top left\")\n",
    "fig.update_layout(xaxis_title=\"TTFT (ms)\", yaxis_title=\"Output tokens per second\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba6d30",
   "metadata": {},
   "source": [
    "This plot should be familiar after your work in the previous notebook. You can see that the `TTFT` increases as the concurrency, displayed next to each dot, goes up. Increasing the concurrency leads to more output tokens per second and better GPU utilization, but `TTFT` is penalized. There are also diminishing gains in throughput when increasing the concurrency.\n",
    "\n",
    "We recommend that you plot the impact of concurrency not only in `TTFT`, but also in inter-token and `E2E Latency`. Likewise, you can also focus on input tokens per second in addition to output tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea75448",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **[EXERCISE] Calculate the Necessary Number of GPUs**\n",
    "\n",
    "The previous plot is useful to understand the tradeoff between latency and throughput in inference. If you choose a high concurrency, you can reach a higher throughput and consequently better GPU utilization. If you choose a low concurrency, the latency is lower. \n",
    "- For online applications where latency is important, a good recommendation is to define a maximum limit for the `TTFT`. For example, you can decide to set a maximum TTFT of 1s for your chatbot application, so that the user waits 1s at most before starting to read the response. You can then choose the highest concurrency level that doesn't surpass that latency. \n",
    "- For offline applications where latency is less of a concern, the concurrency can be higher. We recommend enabling IFB and selecting the highest reasonable concurrency (and ensuring that your connection client is built to sustain it).\n",
    "\n",
    "For a quick exercise, let's suppose we are interested in an online application. We set the maximum `TTFT` to be 1s. In our setup, the highest level of concurrency that doesn't surpass a `TTFT` of 1s is 100, so we can start with that.\n",
    "\n",
    "The number of output tokens per second can be retrieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64360221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a level of concurrency of 100, the output tokens per second is 1806.07\n"
     ]
    }
   ],
   "source": [
    "con = 100\n",
    "file = os.path.join(root_dir, directory_prefix+str(con), f\"profile_export_genai_perf.json\")\n",
    "TPS_100 = parse_data(file, \"output_token_throughput\")\n",
    "print(f\"For a level of concurrency of {con}, the output tokens per second is {TPS_100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff818b01",
   "metadata": {},
   "source": [
    "Next, you can compute the number of prompts per second. Let's assume that your average target prompt is 50 tokens long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8469696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompts per second is 36.12\n"
     ]
    }
   ],
   "source": [
    "## TODO: calculate num prompts per second per GPU\n",
    "PPS = TPS_100 / 50\n",
    "print(f\"The prompts per second is {PPS:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575e4b9",
   "metadata": {},
   "source": [
    "The final step to compute the number of GPUs needed is to estimate the number of requests per second that your system is going to receive. Let's imagine that the estimation is to receive 100 requests per second. Then, assuming that our NIM measurement was obtained with only one GPU, the number of necessary GPUs can be reasonably extrapolated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69ef7c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of GPUs needed to handle 100 requests per second is 2.8\n"
     ]
    }
   ],
   "source": [
    "## TODO: calculate num GPUs needed to handle 100 requests\n",
    "n_gpus = 100 / PPS\n",
    "print(f\"The number of GPUs needed to handle 100 requests per second is {n_gpus:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a432d265",
   "metadata": {},
   "source": [
    "This is how you can estimate the number of GPUs needed for online applications for the specific TP and [NIM model profile](https://docs.nvidia.com/nim/large-language-models/latest/utilities.html#list-available-model-profiles) used. If you get a fractional number of GPUs, you should round up to the nearest TP multiple (i.e. any number with TP1, multiple of 2 with TP2, etc.). You can repeat the measurements above for all the suitable NIM profiles to select the one that best fits your requirements.\n",
    "\n",
    "**In the next notebook,** you will understand how to convert that number of GPUs into a **Total Cost of Ownership (TCO)** metric for both on-prem and in-the-cloud inference deployments.\n",
    "\n",
    "<details>\n",
    "<summary><b>Reveal Solution</b></summary>\n",
    "\n",
    "```python\n",
    "con = 100\n",
    "file = os.path.join(root_dir, directory_prefix+str(con), f\"profile_export_genai_perf.json\")\n",
    "TPS_100 = parse_data(file, \"output_token_throughput\")\n",
    "print(f\"For a level of concurrency of {con}, the output tokens per second is {TPS_100:.2f}\")\n",
    "\n",
    "## TODO: calculate num prompts per second per GPU\n",
    "PPS = TPS_100 / 50\n",
    "print(f\"The prompts per second is {PPS:.2f}\")\n",
    "\n",
    "## TODO: calculate num GPUs needed to handle 100 requests\n",
    "n_gpus = 100 / PPS\n",
    "print(f\"The number of GPUs needed to handle 100 requests per second is {n_gpus:.1f}\")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "Going forward, you can reuse this same methodology for other applications and requirements. For example, instead of focusing on the `TTFT`, you can think about an application that imposes a maximum `E2E Latency`. The same logic applies, but now you need to choose a concurrency level that satisfies your maximum `E2E Latency`. Then, you obtain the output tokens per second for that configuration and can apply the same logic as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f208af1-074e-4ece-9077-67d8fa856679",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ff938-931f-4bb7-aaaa-4d6e72bcca60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d291ea-84ef-4827-b852-033a261f4ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
