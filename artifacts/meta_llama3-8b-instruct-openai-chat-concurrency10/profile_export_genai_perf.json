{
  "request_throughput": {
    "unit": "requests/sec",
    "avg": 11.378172536970691
  },
  "request_latency": {
    "unit": "ms",
    "avg": 878.517281225,
    "p99": 892.6748864799999,
    "p95": 890.33214125,
    "p90": 887.8156607,
    "p75": 879.28195425,
    "p50": 876.681518,
    "p25": 875.8106447499999,
    "max": 892.707809,
    "min": 871.0259629999999,
    "std": 5.0039089996031105
  },
  "time_to_first_token": {
    "unit": "ms",
    "avg": 145.52817012999998,
    "p99": 162.26446499,
    "p95": 161.72384655,
    "p90": 155.1628653,
    "p75": 150.74984824999999,
    "p50": 150.103628,
    "p25": 149.59727775,
    "max": 164.832757,
    "min": 26.047696,
    "std": 23.914673265943286
  },
  "inter_token_latency": {
    "unit": "ms",
    "avg": 12.7529225775,
    "p99": 15.316776419999997,
    "p95": 14.244060199999998,
    "p90": 13.7396198,
    "p75": 13.22351275,
    "p50": 12.7223595,
    "p25": 12.1647955,
    "max": 15.9889,
    "min": 10.835682,
    "std": 0.8564486360476026
  },
  "output_token_throughput": {
    "unit": "tokens/sec",
    "avg": 667.5573827440704
  },
  "output_token_throughput_per_request": {
    "unit": "tokens/sec",
    "avg": 66.78505761717635,
    "p99": 76.34508210494862,
    "p95": 73.29641867358579,
    "p90": 71.96404560108584,
    "p75": 69.47476539542227,
    "p50": 66.34779513861369,
    "p25": 63.90986178660376,
    "max": 78.60869571076424,
    "min": 58.198629249054676,
    "std": 3.8569657018607164
  },
  "output_sequence_length": {
    "unit": "tokens",
    "avg": 58.67,
    "p99": 67.0,
    "p95": 65.0,
    "p90": 63.0,
    "p75": 61.0,
    "p50": 58.0,
    "p25": 56.0,
    "max": 70.0,
    "min": 51.0,
    "std": 3.3735885937677703
  },
  "input_sequence_length": {
    "unit": "tokens",
    "avg": 200.03,
    "p99": 201.01,
    "p95": 200.0,
    "p90": 200.0,
    "p75": 200.0,
    "p50": 200.0,
    "p25": 200.0,
    "max": 202.0,
    "min": 200.0,
    "std": 0.2215851980616034
  },
  "input_config": {
    "model": [
      "meta/llama3-8b-instruct"
    ],
    "model_selection_strategy": "round_robin",
    "backend": "tensorrtllm",
    "endpoint": "v1/chat/completions",
    "endpoint_type": "chat",
    "service_kind": "openai",
    "streaming": true,
    "u": "nim:8000",
    "batch_size": 1,
    "input_dataset": null,
    "num_prompts": 100,
    "output_tokens_mean": 50,
    "output_tokens_mean_deterministic": false,
    "output_tokens_stddev": 0,
    "random_seed": 0,
    "synthetic_input_tokens_mean": 200,
    "synthetic_input_tokens_stddev": 0,
    "concurrency": 10,
    "measurement_interval": 10000,
    "request_rate": null,
    "stability_percentage": 999,
    "artifact_dir": "artifacts/meta_llama3-8b-instruct-openai-chat-concurrency10",
    "generate_plots": false,
    "profile_export_file": "artifacts/meta_llama3-8b-instruct-openai-chat-concurrency10/200_50.json",
    "tokenizer": "hf-internal-testing/llama-tokenizer",
    "verbose": false,
    "subcommand": null,
    "prompt_source": "synthetic",
    "formatted_model_name": "meta/llama3-8b-instruct",
    "extra_inputs": {
      "max_tokens": 50,
      "min_tokens": 50,
      "ignore_eos": true
    }
  }
}