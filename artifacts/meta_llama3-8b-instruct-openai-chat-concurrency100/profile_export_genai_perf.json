{
  "request_throughput": {
    "unit": "requests/sec",
    "avg": 30.935429942267838
  },
  "request_latency": {
    "unit": "ms",
    "avg": 3228.72631934,
    "p99": 3304.80054971,
    "p95": 3297.1413713499996,
    "p90": 3296.4963820999997,
    "p75": 3291.404599,
    "p50": 3280.8441209999996,
    "p25": 3147.28909225,
    "max": 3305.6335639999998,
    "min": 3092.077313,
    "std": 72.7313638164219
  },
  "time_to_first_token": {
    "unit": "ms",
    "avg": 1019.6901196972727,
    "p99": 1564.26834917,
    "p95": 1424.54508275,
    "p90": 1419.9281885999999,
    "p75": 1303.30015475,
    "p50": 1222.6414144999999,
    "p25": 647.85233025,
    "max": 1564.9647069999999,
    "min": 118.95414099999999,
    "std": 334.44751601765876
  },
  "inter_token_latency": {
    "unit": "ms",
    "avg": 38.61366689454545,
    "p99": 50.89024096999999,
    "p95": 48.817544299999994,
    "p90": 47.213865799999994,
    "p75": 44.38181075,
    "p50": 36.992419,
    "p25": 33.17027425,
    "max": 59.670462,
    "min": 26.093569,
    "std": 6.308892857415114
  },
  "output_token_throughput": {
    "unit": "tokens/sec",
    "avg": 1806.0666462658548
  },
  "output_token_throughput_per_request": {
    "unit": "tokens/sec",
    "avg": 18.090811181027064,
    "p99": 20.66546118108134,
    "p95": 19.834571882587102,
    "p90": 19.460465849768738,
    "p75": 18.822548913527996,
    "p50": 18.087917363573375,
    "p25": 17.322551133851377,
    "max": 22.33327739347186,
    "min": 15.768723647894682,
    "std": 1.0623163786388659
  },
  "output_sequence_length": {
    "unit": "tokens",
    "avg": 58.38181818181818,
    "p99": 66.00999999999999,
    "p95": 64.0,
    "p90": 63.0,
    "p75": 60.0,
    "p50": 58.0,
    "p25": 56.0,
    "max": 70.0,
    "min": 51.0,
    "std": 3.1971061295030205
  },
  "input_sequence_length": {
    "unit": "tokens",
    "avg": 200.03,
    "p99": 201.01,
    "p95": 200.0,
    "p90": 200.0,
    "p75": 200.0,
    "p50": 200.0,
    "p25": 200.0,
    "max": 202.0,
    "min": 200.0,
    "std": 0.2215851980616034
  },
  "input_config": {
    "model": [
      "meta/llama3-8b-instruct"
    ],
    "model_selection_strategy": "round_robin",
    "backend": "tensorrtllm",
    "endpoint": "v1/chat/completions",
    "endpoint_type": "chat",
    "service_kind": "openai",
    "streaming": true,
    "u": "nim:8000",
    "batch_size": 1,
    "input_dataset": null,
    "num_prompts": 100,
    "output_tokens_mean": 50,
    "output_tokens_mean_deterministic": false,
    "output_tokens_stddev": 0,
    "random_seed": 0,
    "synthetic_input_tokens_mean": 200,
    "synthetic_input_tokens_stddev": 0,
    "concurrency": 100,
    "measurement_interval": 10000,
    "request_rate": null,
    "stability_percentage": 999,
    "artifact_dir": "artifacts/meta_llama3-8b-instruct-openai-chat-concurrency100",
    "generate_plots": false,
    "profile_export_file": "artifacts/meta_llama3-8b-instruct-openai-chat-concurrency100/200_50.json",
    "tokenizer": "hf-internal-testing/llama-tokenizer",
    "verbose": false,
    "subcommand": null,
    "prompt_source": "synthetic",
    "formatted_model_name": "meta/llama3-8b-instruct",
    "extra_inputs": {
      "max_tokens": 50,
      "min_tokens": 50,
      "ignore_eos": true
    }
  }
}